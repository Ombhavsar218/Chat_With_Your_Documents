# -*- coding: utf-8 -*-
"""Chat_with_docments2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YLIU9hVZVZk24EaFMTXEriat9rmjmwS2
"""

!pip install gradio

!pip install python-dotenv

!pip install langchain_community

!pip install groq

pip install unstructured

pip install faiss-cpu

pip install python-docx

import os
import time
import gradio as gr
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import TextLoader, CSVLoader
from langchain.docstore.document import Document as LangchainDocument
from docx import Document
from groq import Groq

# Custom DOCX loader
class CustomDocxLoader:
    def __init__(self, file_path):
        self.file_path = file_path

    def load(self):
        doc = Document(self.file_path)
        return [LangchainDocument(page_content="\n".join([p.text for p in doc.paragraphs]))]

# Load environment variables
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

if not GROQ_API_KEY:
    raise EnvironmentError("GROQ_API_KEY is missing!!")

# Initialize GROQ API client
client = Groq(api_key=GROQ_API_KEY)

# Function to query GROQ API
def groq_api_query(query, context, retries=3, delay=5):
    url = "https://api.groq.co/query"  # Confirm this URL with the correct API endpoint
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "query": query,
        "context": context
    }

    attempt = 0
    while attempt < retries:
        try:
            # Send a POST request to the GROQ API
            response = client.chat.completions.create(
                messages=[{"role": "user", "content": f"{context}\n\nQuestion: {query}"}],
                model="llama3-8b-8192"  # Change the model as needed
            )
            return response.choices[0].message.content  # Return the response from the model

        except Exception as e:
            # If the request fails, print the error and retry
            attempt += 1
            print(f"Attempt {attempt}/{retries} failed: {str(e)}")
            if attempt < retries:
                time.sleep(delay)
            else:
                return f"Error: {str(e)}. Max retries exceeded."

# Function to process documents and query GROQ API
def process_file_and_query(file, query):
    try:
        # Detect file type and load content
        if file.name.endswith('.pdf'):
            loader = PyPDFLoader(file.name)
        elif file.name.endswith('.docx'):
            loader = CustomDocxLoader(file.name)
        elif file.name.endswith('.txt'):
            loader = TextLoader(file.name)
        elif file.name.endswith('.csv'):
            loader = CSVLoader(file.name)
        else:
            return "Unsupported file format. Please upload PDF, DOCX, TXT, or CSV files."

        raw_documents = loader.load()

        # Normalize documents into a list of LangchainDocument objects
        documents = [
            LangchainDocument(page_content=doc.page_content if hasattr(doc, "page_content") else doc["page_content"])
            for doc in raw_documents
        ]

        # Split the text into chunks for embedding
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(documents)

        # Embed the documents
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")
        vector_store = FAISS.from_documents(chunks, embeddings)

        # Retrieve relevant documents
        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 4})
        retrieved_docs = retriever.get_relevant_documents(query)
        context = "\n\n".join([doc.page_content for doc in retrieved_docs])

        # Query the GROQ API for an answer based on the context
        answer = groq_api_query(query, context)

        return answer
    except Exception as e:
        return f"Error processing file: {str(e)}"

# Gradio interface function
def rag_interface(file, query):
    if not file:
        return "Please upload a file."
    if not query:
        return "Please enter a query."
    return process_file_and_query(file, query)

# Function to clear inputs and outputs
def clear_inputs():
    return None, None, "Your answer will appear here."

# Set up Gradio interface with modified layout
with gr.Blocks(css="""
    #clear_button {background-color: red; color: white;}
    #answer_button {background-color: green; color: white;}
""") as demo:
    with gr.Row():
        with gr.Column(scale=1):  # File upload on the left side
            file_input = gr.File(label="Upload your file")

        with gr.Column(scale=2):  # Right side for output, query, and result
            output = gr.Textbox(label="Answer", interactive=False, value="Your answer will appear here.")
            query_input = gr.Textbox(label="Enter your query", placeholder="Ask a question based on the document.")
            with gr.Row():
                clear_button = gr.Button("Clear", elem_id="clear_button")
                answer_button = gr.Button("Get Answer", elem_id="answer_button")

    # Link buttons to functions
    answer_button.click(fn=rag_interface, inputs=[file_input, query_input], outputs=output)
    clear_button.click(fn=clear_inputs, inputs=[], outputs=[file_input, query_input, output])

# Run the app
if __name__ == "__main__":
    demo.launch(share=True)

